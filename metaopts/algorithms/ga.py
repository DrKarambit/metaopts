import tensorflow as tf


def ga(model_weights, model_fitness, generation_limit, fitness_limit, population_size, elite_size, crossover_rate, mutation_rate):
    """
    Implementation of a Genetic Algorithm.
    The algorithm uses elitism, roulette wheel selection, multi-point crossover and gaussian mutation.

    Args:
        model_weights: `list` of `tf.Variable` - List of model weights.
        model_fitness: `tf.function` - Fitness function generated by `metaopts.create_fitness`.
        generation_limit: `int` - Maximum number of generations.
        fitness_limit: `float` - Fitness value threshold.
        population_size: `int` - Number of individuals in the population.
        elite_size: `int` - Number of elite individuals.
        crossover_rate: `float` - Crossover probability.
        mutation_rate: `float` - Mutation probability.
    """

    @tf.function
    def update_fitness():

        # Print debug information
        print('Tracing update_fitness...')

        # Loop over individuals
        for i in range(population_size):

            # Loop over weights
            for mw, p in zip(model_weights, population):

                # Assign test weights to model
                mw.assign(p[i])
            
            # Update fitness value
            fitness_values[i].assign(model_fitness())

    @tf.function
    def sort_population():

        # Print debug information
        print('Tracing sort_population...')

        # Sort fitness values in ascending order
        order = tf.argsort(fitness_values)

        # Assign new order to fitness values and the population
        fitness_values.assign(tf.gather(fitness_values, order, axis=0))
        for p in population:
            p.assign(tf.gather(p, order, axis=0))

    @tf.function
    def roulette_wheel_selection():

        # Print debug information
        print('Tracing roulette_wheel_selection...')

        # Get best fitness values
        elite = fitness_values[:elite_size]

        # Reverse ratios to fit minimization problem
        reversed = tf.reduce_sum(elite) / elite

        # Prepare logits for `tf.random.categorical`
        logits = tf.math.log(reversed)

        # Repeat logits one more time to get two parents
        repeated = tf.repeat([logits], 2, axis=0)

        # Select parents
        selected = tf.random.categorical(repeated, selection_size, dtype=tf.int32)

        # Assign selected parents
        parents.assign(selected)

    @tf.function
    def multipoint_crossover():

        # Print debug information
        print('Tracing multipoint_crossover...')

        # Loop over weights
        for p in population:

            # Shape of randomly generated crossover points
            crossover_shape = tf.concat([[selection_size], p.shape[1:]], 0)

            # Generate crossover points
            crossover_points = tf.random.uniform(crossover_shape, 0, 1) < crossover_rate

            # Get parents
            parent1 = tf.gather(p, parents[0], axis=0)
            parent2 = tf.gather(p, parents[1], axis=0)

            # Assign child weights to population
            p[elite_size:].assign(tf.where(crossover_points, parent1, parent2))

    @tf.function
    def gaussian_mutation():

        # Print debug information
        print('Tracing gaussian_mutation...')

        # Loop over weights
        for p in population:

            # Shape of randomly generated mutation points
            mutation_shape = tf.concat([[selection_size], p.shape[1:]], 0)

            # Generate mutation points
            mutation_points = tf.random.uniform(mutation_shape, 0, 1) < mutation_rate

            # Generate mutation values
            mutation_values = tf.random.normal(mutation_shape, 1, 0.1)

            # Assign mutated weights to population
            p[elite_size:].assign(tf.where(mutation_points, mutation_values * p[elite_size:], p[elite_size:]))


    # Number of individuals to be generated by crossover
    selection_size = population_size - elite_size
    
    # Initialize population by copying model weights
    population = [tf.Variable(tf.repeat([weights], population_size, axis=0)) for weights in model_weights]
    
    # Initialize fitness value vector with zeros
    fitness_values = tf.Variable(tf.zeros(population_size, dtype=tf.float32))
    
    # Initialize parent matrix with zeros
    parents = tf.Variable(tf.zeros((2, selection_size), dtype=tf.int32))
    
    # Update fitness values and sort population
    update_fitness()
    sort_population()

    # Print debug information
    print('Starting Genetic Algorithm.')

    # Initialize generation counter
    gen = 0

    # Loop until fitness limit is reached or generation limit is exceeded
    while fitness_values[0] > fitness_limit and gen < generation_limit:
        
        # Increment generation counter
        gen += 1

        # Call genetic operators
        roulette_wheel_selection()
        multipoint_crossover()
        gaussian_mutation()

        # Update fitness values and sort population
        update_fitness()
        sort_population()

        # Print debug information
        print('Generation: {0}\tBest fitness: {1}'.format(gen, fitness_values[0].numpy()), end='\r')

    # Print debug information
    print('\nGenetic Algorithm finished.')

    # Apply best solution to model
    for mw, p in zip(model_weights, population):
        mw.assign(p[0])

    # Print debug information
    print('Best solution applied to model.')
