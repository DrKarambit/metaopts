import tensorflow as tf
from metaopts import create_population, sort_population, update_population_fitness


def ga(
        model_weights,
        model_fitness_fn,
        generation_limit,
        fitness_limit,
        population_size,
        elite_size,
        transfer_learning=False,
        crossover_rate=0.2,
        mutation_rate=0.2
    ):
    """
    Implementation of a Genetic Algorithm.

    Args:
        model_weights: `list` of `tf.Variable` - List of model weights.
        model_fitness_fn: `tf.function` - Fitness function generated by `metaopts.create_fitness`.
        generation_limit: `int` - Maximum number of generations.
        fitness_limit: `float` - Fitness value threshold.
        population_size: `int` - Number of individuals in the population.
        elite_size: `int` - Number of elite individuals.
        transfer_learning: `bool` - Whether to use transfer learning.
        crossover_rate: `float` - Crossover probability.
        mutation_rate: `float` - Mutation probability.
    
    Notes:

    * The algorithm uses elitism, roulette wheel selection, multi-point crossover and gaussian mutation.
    """

    @tf.function
    def roulette_wheel_selection():

        # Print debug information
        print('Tracing roulette_wheel_selection...')

        # Get best fitness values
        elite = fitness_values[:elite_size]

        # Reverse ratios to fit minimization problem
        reversed = tf.reduce_sum(elite) / elite

        # Prepare logits for `tf.random.categorical`
        logits = tf.math.log(reversed)

        # Repeat logits one more time to get two parents
        repeated = tf.repeat([logits], 2, axis=0)

        # Select parents
        selected = tf.random.categorical(repeated, selection_size, dtype=tf.int32)

        # Assign selected parents
        parents.assign(selected)

    @tf.function
    def multipoint_crossover():

        # Print debug information
        print('Tracing multipoint_crossover...')

        # Loop over weights
        for p in population:

            # Shape of randomly generated crossover points
            crossover_shape = tf.concat([[selection_size], p.shape[1:]], 0)

            # Generate crossover points
            crossover_points = tf.random.uniform(crossover_shape, 0, 1) < crossover_rate

            # Get parents
            parent1 = tf.gather(p, parents[0], axis=0)
            parent2 = tf.gather(p, parents[1], axis=0)

            # Assign child weights to population
            p[elite_size:].assign(tf.where(crossover_points, parent1, parent2))

    @tf.function
    def gaussian_mutation():

        # Print debug information
        print('Tracing gaussian_mutation...')

        # Loop over weights
        for p in population:

            # Shape of randomly generated mutation points
            mutation_shape = tf.concat([[selection_size], p.shape[1:]], 0)

            # Generate mutation points
            mutation_points = tf.random.uniform(mutation_shape, 0, 1) < mutation_rate

            # Generate mutation values
            mutation_values = tf.random.normal(mutation_shape, 1, 0.1)

            # Assign mutated weights to population
            p[elite_size:].assign(tf.where(mutation_points, mutation_values * p[elite_size:], p[elite_size:]))

    
    # Initialize population
    population = create_population(model_weights, population_size, transfer_learning)
    
    # Initialize fitness value vector with zeros
    fitness_values = tf.Variable(tf.zeros(population_size, dtype=tf.float32))
    
    # Number of individuals to be generated by crossover
    selection_size = population_size - elite_size

    # Initialize parent matrix with zeros
    parents = tf.Variable(tf.zeros((2, selection_size), dtype=tf.int32))
    
    # Update fitness values and sort population
    update_population_fitness(model_weights, model_fitness_fn, fitness_values, population, population_size)
    sort_population(population, fitness_values)

    # Print debug information
    print('Starting Genetic Algorithm.')

    # Initialize generation counter
    gen = 0

    # Loop until fitness limit is reached or generation limit is exceeded
    while fitness_values[0] > fitness_limit and gen < generation_limit:
        
        # Increment generation counter
        gen += 1

        # Call genetic operators
        roulette_wheel_selection()
        multipoint_crossover()
        gaussian_mutation()

        # Update fitness values and sort population
        update_population_fitness(model_weights, model_fitness_fn, fitness_values, population, population_size)
        sort_population(population, fitness_values)

        # Print debug information
        print('Generation: {0} Best fitness: {1}'.format(gen, fitness_values[0].numpy()), end='\r')

    # Print debug information
    print('\nGenetic Algorithm finished.')

    # Apply best solution to the model
    for mw, p in zip(model_weights, population):
        mw.assign(p[0])

    # Print debug information
    print('Best solution applied to model.')
