"""Genetic Algorithm."""

import tensorflow as tf
import metaopts.utilities as mou


def ga(
        model_weights,
        model_fitness_fn,
        generation_limit,
        fitness_limit,
        population_size,
        elite_size,
        transfer_learning=False,
        fitness_log_frequency=-1,
        best_individual_save_frequency=-1,
        learning_rate=0.1,
        crossover_rate=0.2,
        mutation_rate=0.2
    ):
    """
    Implementation of a Genetic Algorithm.

    Args:
        model_weights: `list` of `tf.Variable` - List of model weights.
        model_fitness_fn: `tf.function` - Fitness function generated by `metaopts.create_fitness`.
        generation_limit: `int` - Maximum number of generations.
        fitness_limit: `float` - Fitness value threshold.
        population_size: `int` - Number of individuals in the population.
        elite_size: `int` - Number of elite individuals.
        transfer_learning: `bool` - Whether to use transfer learning.
        fitness_log_frequency: `int` - Frequency of logging fitness values to the log file. If set to -1, no logging is performed.
        best_individual_save_frequency: `int` - Frequency of saving the best individual to a pickle file. If set to -1, no saving is performed.
        learning_rate: `float` - Learning rate.
        crossover_rate: `float` - Crossover probability.
        mutation_rate: `float` - Mutation probability.
    
    Notes:

    * The algorithm uses elitism, roulette wheel selection, multi-point crossover and gaussian mutation.
    """

    @tf.function
    def roulette_wheel_selection():

        # Print debug information
        mou.print_function_trace('roulette_wheel_selection')

        # Get best fitness values
        elite = fitness_values[:elite_size]

        # Reverse ratios to fit minimization problem
        reversed = tf.reduce_sum(elite) / elite

        # Prepare logits for `tf.random.categorical`
        logits = tf.math.log(reversed)

        # Repeat logits one more time to get two parents
        repeated = tf.repeat([logits], 2, axis=0)

        # Select parents
        selected = tf.random.categorical(repeated, selection_size, dtype=tf.int32)

        # Assign selected parents
        parents.assign(selected)

    @tf.function
    def multipoint_crossover():

        # Print debug information
        mou.print_function_trace('multipoint_crossover')

        # Loop over weights
        for p in population:

            # Shape of randomly generated crossover points
            crossover_shape = tf.concat([[selection_size], p.shape[1:]], 0)

            # Generate crossover points
            crossover_points = tf.random.uniform(crossover_shape, 0, 1) < crossover_rate

            # Get parents
            parent1 = tf.gather(p, parents[0], axis=0)
            parent2 = tf.gather(p, parents[1], axis=0)

            # Assign child weights to population
            p[elite_size:].assign(tf.where(crossover_points, parent1, parent2))

    @tf.function
    def gaussian_mutation():

        # Print debug information
        mou.print_function_trace('gaussian_mutation')

        # Loop over weights
        for p in population:

            # Shape of randomly generated mutation points
            mutation_shape = tf.concat([[selection_size], p.shape[1:]], 0)

            # Generate mutation points
            mutation_points = tf.random.uniform(mutation_shape, 0, 1) < mutation_rate

            # Generate mutation values
            mutation_values = tf.random.normal(mutation_shape, 1, learning_rate)

            # Assign mutated weights to population
            p[elite_size:].assign(tf.where(mutation_points, mutation_values * p[elite_size:], p[elite_size:]))

    
    # Initialize population
    population = mou.create_population(
        model_weights=model_weights,
        population_size=population_size,
        transfer_learning=transfer_learning
    )
    
    # Initialize fitness value vector with zeros
    fitness_values = tf.Variable(tf.zeros(population_size, dtype=tf.float32))
    
    # Number of individuals to be generated by crossover
    selection_size = population_size - elite_size

    # Initialize parent matrix with zeros
    parents = tf.Variable(tf.zeros((2, selection_size), dtype=tf.int32))
    
    # Update fitness values and sort population
    mou.update_population_fitness(
        model_weights,
        model_fitness_fn,
        fitness_values,
        population,
        population_size
    )
    mou.sort_population(
        population,
        fitness_values
    )

    # Print debug information
    algo_name = 'Genetic Algorithm'
    mou.print_algo_start(algo_name)

    # Initialize generation counter
    gen = tf.Variable(0, dtype=tf.int32)

    # Loop until fitness limit is reached or generation limit is exceeded
    while fitness_values[0] > fitness_limit and gen < generation_limit:
        
        # Increment generation counter
        gen.assign_add(1)

        # Call genetic operators
        roulette_wheel_selection()
        multipoint_crossover()
        gaussian_mutation()

        # Update fitness values and sort population
        mou.update_population_fitness(
            model_weights,
            model_fitness_fn,
            fitness_values,
            population,
            population_size
        )
        mou.sort_population(
            population,
            fitness_values
        )

        # Log fitness
        if fitness_log_frequency > 0:
            mou.log_fitness_value(
                fitness_value=float(fitness_values[0]),
                log_file_name='{0} fitness'.format(algo_name),
                max_cache_size=fitness_log_frequency
            )

        # Save best individual
        if best_individual_save_frequency > 0 and gen % best_individual_save_frequency == 0:
            mou.save_individual(
                population=population,
                individual_index=tf.argmin(fitness_values),
                file_path='{0} weights'.format(algo_name)
            )

        # Print training information
        mou.print_training_status(
            generation=int(gen),
            generation_limit=int(generation_limit),
            best_fitness_value=float(fitness_values[0])
        )


    # Print debug information
    mou.print_algo_end(algo_name)

    # Apply best solution to the model
    mou.apply_best_solution(
        model_weights,
        model_fitness_fn,
        fitness_values,
        population,
        population_size
    )

    # Log fitness
    if fitness_log_frequency > 0:
        mou.log_fitness_value(
            fitness_value=float(fitness_values[0]),
            log_file_name='{0} fitness'.format(algo_name),
            max_cache_size=fitness_log_frequency,
            force_file_write=True
        )

    # Save best individual
    if best_individual_save_frequency > 0:
        mou.save_individual(
            population=population,
            individual_index=tf.argmin(fitness_values),
            file_path='{0} weights'.format(algo_name)
        )
